{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Part 1: Implementing a feedforward neural network using NumPy\n",
    "\n",
    "We'll implement a neural network on the MNIST dataset -- from scratch only using the Numpy library. That will allow us to build an intuition of how our model actually works beyond the surface level PyTorch implementation that we'll do later in this project. \n",
    "\n",
    "Like I said, this will be a feedforward neural network, meaning information only flows forward during inference. To do this, we'll make use of dense layers, the ReLU activation function, categorical cross-entropy loss, and a softmax activation function for the output. I've made a little diagram of what we're going to be implementing below:\n",
    "> INSERT IMG\n",
    "\n",
    "I'll work on everything in sections, implementing it with example classes in this jupyter notebook, and adding the real classes that we'll use into the classes.py file in this same folder (filepath: fnn/p1-numpy/classes.py).\n",
    "\n",
    "As a precursor to all our work, let's initialize the necessary packages in the workspace. "
   ],
   "id": "27bc1f75b0927bc9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T01:48:32.228175Z",
     "start_time": "2025-06-26T01:48:32.130638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Package Initialization\n",
    "import numpy as np\n",
    "from classes import DenseLayer, ReLU, Softmax"
   ],
   "id": "1e65ce53eb3e3a81",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we can begin the implementation :)\n",
    "\n",
    "Dense layers, otherwise known as fully-connected layers, are the foundation of neural networks. A dense layer is really quite simple, if you understand the workings of matrix multiplication. I'll briefly go over it, but if you need a better introduction to dense layers I would recommend the book *Neural Networks from Scratch* by Kinsley and Kukie≈Ça.\n",
    "\n",
    "# Section 1: The Forward Pass of Dense Layers\n",
    "\n",
    "At the core level, the way a neural network layer works is matrix multiplication. We can work through an example for this. Let's think of this as creating a function where we input a list X, of length 3 with integers, and it outputs a list Y, of length 4 with sum(X) in each entry. You can look at the diagram below for an example:\n",
    "\n",
    "<br/>\n",
    "\n",
    "<img src=\"../../diagrams/ex1.png\" alt=\"A diagram showing our proposed 'function.'\"/>\n",
    "\n",
    "Skip this if you understand matrix multiplication: effectively, what we're doing is just a function that carries out matrix multiplication. Matrix multiplication is done by the element wise multiplication of two matrices of sizes (n x m) and (m x p), respectively. As you can see there, that middle dimension adding up is key, and that's because we're doing row x column. I won't go too into the weeds there, but let's look at a mini-example -- almost recursive if that makes sense :). Let's say we're trying to multiply a matrix X of size (1 x 2) times another matrix Y of size (2 x 2). That can be done because the middle dimension lines up. If matrix X is [a, b] and matrix Y is [[c, d],[e, f]]. That is, where every column of matrix Y represents the weights of one neuron as a column vector. Then, what matrix multiplication is doing is just [(a\\*c + a\\*e), (b\\*d + b\\*f)]. That's it for this recursive example, and I hope that makes sense.\n",
    "\n",
    "Now, I'll quickly use Numpy to show the main example I'm referring to above. Please note, np.array() is the standard way for creating arrays or matrices in numpy. "
   ],
   "id": "901a6e2208f729b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T23:53:43.938345Z",
     "start_time": "2025-06-25T23:53:43.930773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create the input array of size (1 x 3)\n",
    "inputs = np.array([1., 2., 4.])\n",
    "# Create the weight array of size (3 x 4) as we have 3 inputs and 4 desired outputs\n",
    "weights = np.array([[1., 1., 1., 1.],\n",
    "                    [1., 1., 1., 1.],\n",
    "                    [1., 1., 1., 1.]])\n",
    "\n",
    "# Perform matrix multiplication on the inputs and weights, note np.dot() is the standard and combined way of performing both dot products (which are normally for vectors) and matrix multiplication (which is for matrices)\n",
    "outputs = np.dot(inputs, weights)\n",
    "\n",
    "print(f\"The matrix mult. output is: {outputs}\")"
   ],
   "id": "f0f6b13e50370a9e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The matrix mult. output is: [7. 7. 7. 7.]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "That should about give you an intuition of how a neural network layer really works, but there's one more thing to address in this part of things: biases. \n",
    "\n",
    "Biases are parameters which are trainable and which are meant to offset the outputs positively or negatively. Each neuron has its own bias parameter, which we usually initialize biases to 0 and then modify according to the gradients during training. It's really quite a simple concept and there's not much more to it.\n",
    "\n",
    "In practice, these biases are added as scalars (integers) to the product of each neuron. Let's look at how this happens, using our inputs and weights from the previous code cell."
   ],
   "id": "bd6a70a6ed24e848"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T00:22:43.205424Z",
     "start_time": "2025-06-26T00:22:43.196390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# INPUTS AND WEIGHTS ARE PRE-LOADED FROM THE EARLIER CELL\n",
    "\n",
    "# NEW: create the biases \n",
    "biases = np.array([1., 2., 3., 4.])\n",
    "\n",
    "# Perform matrix multiplication on the inputs and weights, note np.dot() is the standard and combined way of performing both dot products (which are normally for vectors) and matrix multiplication (which is for matrices)\n",
    "outputs = np.dot(inputs, weights)\n",
    "\n",
    "print(f\"The outputs before adding biases are: {outputs}\")\n",
    "\n",
    "outputs += biases\n",
    "\n",
    "print(f\"The outputs after adding biases are: {outputs}\")"
   ],
   "id": "d25c038fa33f2efd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The outputs before adding biases are: [7. 7. 7. 7.]\n",
      "The outputs after adding biases are: [ 8.  9. 10. 11.]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now, we understand how the basics of how the whole thing works, at least in the forward pass. All in all, each neuron is really just carrying out ```y=mx+b``` where m is the input, x is the weight, b is the neuron's bias, and the result y is the neuron's output.\n",
    "\n",
    "Great, so we can now implement the forward class of our dense layer. Like I said above, I'll implement an example class here but all the changes will be saved in the corresponding classes.py file."
   ],
   "id": "c0bb05e32d2f80bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T00:39:50.824437Z",
     "start_time": "2025-06-26T00:39:50.817365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class exDenseLayer:\n",
    "    # Initialization method to initialize a dense layer object\n",
    "    def __init__(self, numInputs, numNeurons):\n",
    "        # Create random weights and scale them down, of size (inputs x neurons)\n",
    "        self.weights = 0.01 * np.random.randn(numInputs, numNeurons)\n",
    "        # Create an array filled with 0's with a bias for each neuron\n",
    "        self.biases = np.zeros((1, numNeurons))\n",
    "    \n",
    "    # Our forward pass method\n",
    "    def forward(self, inputs):\n",
    "        # Calculate outputs using the method previously discussed\n",
    "        self.outputs = np.dot(inputs, self.weights) + self.biases"
   ],
   "id": "fc1a48999366a609",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "That's about everything we need for a forward pass! Now we get to talk about the ReLU activation function!\n",
    "\n",
    "## Section 2: The Various Activation Functions\n",
    "Activation functions are necessary parts of neural networks if we ever want to model non-linear functions. In this section, we'll cover two of them: ReLU and Softmax.\n",
    "\n",
    "### Section 2.1: The ReLU Activation Function\n",
    "\n",
    "ReLU stands for \"rectified linear unit\" and is a piecewise activation function that makes the best of all worlds. It basically works using the following equation:\n",
    "$$\n",
    "f(x) = \n",
    "\\begin{cases}\n",
    "    x & \\text{ if } x > 0 \\\\\n",
    "    0 & \\text{ otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Let's implement a class for our ReLU activation function, making use of the properties of Numpy to simplify our work."
   ],
   "id": "10068d7f1f0cdfdb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T01:12:07.327763Z",
     "start_time": "2025-06-26T01:12:07.317633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class exReLU:\n",
    "    # Our forward method; no overriding init method needed\n",
    "    def forward(self, inputs):\n",
    "        # Using the np.maximum() method to carry out our comparisons \n",
    "        self.output = np.maximum(0, inputs)"
   ],
   "id": "cfc3d12dc36f305b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, let's try it out on an example array to see it in action. Spoiler, it's nothing crazy.",
   "id": "eeec50d0ecc7cfef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T01:12:16.662523Z",
     "start_time": "2025-06-26T01:12:16.657080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a random array\n",
    "exOutput = np.array([[-0.5, 0.3],\n",
    "                   [0.43, -0.1]])\n",
    "\n",
    "# Instantiate a ReLU object\n",
    "activation = ReLU()\n",
    "\n",
    "# Use the ReLU on our example outputs\n",
    "activation.forward(exOutput)\n",
    "\n",
    "print(activation.output)"
   ],
   "id": "3fd9813cb50af695",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.3 ]\n",
      " [0.43 0.  ]]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As we can see, the values less than 0 have been zerod out, whereas all the non-negative values have remained as they were!\n",
    "\n",
    "And that's all there is to ReLU. It basically zeros out values less than 0 and keeps values greater than or equal to 0. Now we can move to the second activation function we'll look at: Softmax.\n",
    "\n",
    "### Section 2.2: Softmax\n",
    "\n",
    "The Softmax activation function is the one that we'll be using on our outputs. The Softmax is used in situations where we're to perform some kind of categorical operations. It works by taking the outputs and creating probabilities for each output class that ultimately add up to one -- giving us the \"most likely\" class.\n",
    "\n",
    "The function for the Softmax function is:\n",
    "$$\n",
    "\\frac{e^{z_{i,j}}}{\\sum_{l=1}^{L} e^{z_{i,l}}}\n",
    "$$\n",
    "\n",
    "All that really means is it's getting the normalized probabilities for each, with it all adding up to 1.\n",
    "\n",
    "So, let's implement this in a class. "
   ],
   "id": "778d95ac6b7bdc85"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T01:45:08.652602Z",
     "start_time": "2025-06-26T01:45:08.631727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class exSoftmax:\n",
    "    # Method for the forward pass; no init overriding needed\n",
    "    def forward(self, inputs):\n",
    "        # Calculate the non-normalized probabilities using np.exp(), which exponentiates everything, np.max() which finds the maximum value in the matrix, and np.sum() which performs a summation of each row/col as desired\n",
    "        unProb = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        # Normalize the inputs for every sample\n",
    "        nProb = unProb / np.sum(unProb, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = nProb"
   ],
   "id": "e3b27e35ce7d63d0",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "With that implemented, let's demonstrate how it works! ",
   "id": "e54745119ac4d262"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T01:52:16.374896Z",
     "start_time": "2025-06-26T01:52:16.368427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create an array with hypothetical outputs\n",
    "exOutput = np.array([[1, 4, 5]])\n",
    "\n",
    "# Instantiate our Softmax object\n",
    "activation = Softmax()\n",
    "\n",
    "# Run our output through a Softmax layer\n",
    "activation.forward(exOutput)\n",
    "\n",
    "print(f\"The normalized output probabilities are: {activation.output}\")"
   ],
   "id": "36d3c77d25ac4a2f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The normalized output probabilities are: [[0.01321289 0.26538793 0.72139918]]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Awesome, and that's really it for our Softmax. There's not much more we need to know for the purposes of this implementation. Now, we're going to move to our method of measuring accuracy: the categorical cross-entropy loss.\n",
    "\n",
    "## Section 3: Categorical Cross-Entropy Loss "
   ],
   "id": "f842eefdcd9762e3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
