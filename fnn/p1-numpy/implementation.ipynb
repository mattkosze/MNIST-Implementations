{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Part 1: Implementing a feedforward neural network using NumPy\n",
    "\n",
    "We'll implement a neural network on the MNIST dataset -- from scratch only using the Numpy library. That will allow us to build an intuition of how our model actually works beyond the surface level PyTorch implementation that we'll do later in this project. \n",
    "\n",
    "Like I said, this will be a feedforward neural network, meaning information only flows forward during inference. To do this, we'll make use of dense layers, the ReLU activation function, categorical cross-entropy loss, and a softmax activation function for the output. I've made a little diagram of what we're going to be implementing below:\n",
    "> INSERT IMG\n",
    "\n",
    "I'll work on everything in sections, implementing it with example classes in this jupyter notebook, and adding the real classes that we'll use into the classes.py file in this same folder (filepath: fnn/p1-numpy/classes.py).\n",
    "\n",
    "As a precursor to all our work, let's initialize the necessary packages in the workspace. "
   ],
   "id": "27bc1f75b0927bc9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T23:03:26.558799Z",
     "start_time": "2025-06-25T23:03:25.747582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Package Initialization\n",
    "import numpy as np"
   ],
   "id": "1e65ce53eb3e3a81",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we can begin the implementation :)\n",
    "\n",
    "Dense layers, otherwise known as fully-connected layers, are the foundation of neural networks. A dense layer is really quite simple, if you understand the workings of matrix multiplication. I'll briefly go over it, but if you need a better introduction to dense layers I would recommend the book *Neural Networks from Scratch* by Kinsley and Kukie≈Ça.\n",
    "\n",
    "# Section 1: The Forward Pass of Dense Layers\n",
    "\n",
    "At the core level, the way a neural network layer works is matrix multiplication. We can work through an example for this. Let's think of this as creating a function where we input a list X, of length 3 with integers, and it outputs a list Y, of length 4 with sum(X) in each entry. You can look at the diagram below for an example:\n",
    "\n",
    "<br/>\n",
    "\n",
    "<img src=\"../../diagrams/ex1.png\" alt=\"A diagram showing our proposed 'function.'\"/>\n",
    "\n",
    "Skip this if you understand matrix multiplication: effectively, what we're doing is just a function that carries out matrix multiplication. Matrix multiplication is done by the element wise multiplication of two matrices of sizes (n x m) and (m x p), respectively. As you can see there, that middle dimension adding up is key, and that's because we're doing row x column. I won't go too into the weeds there, but let's look at a mini-example -- almost recursive if that makes sense :). Let's say we're trying to multiply a matrix X of size (1 x 2) times another matrix Y of size (2 x 2). That can be done because the middle dimension lines up. If matrix X is [a, b] and matrix Y is [[c, d],[e, f]]. That is, where every column of matrix Y represents the weights of one neuron as a column vector. Then, what matrix multiplication is doing is just [(a\\*c + a\\*e), (b\\*d + b\\*f)]. That's it for this recursive example, and I hope that makes sense.\n",
    "\n",
    "Now, I'll quickly use Numpy to show the main example I'm referring to above. Please note, np.array() is the standard way for creating arrays or matrices in numpy. "
   ],
   "id": "901a6e2208f729b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T23:53:43.938345Z",
     "start_time": "2025-06-25T23:53:43.930773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create the input array of size (1 x 3)\n",
    "inputs = np.array([1., 2., 4.])\n",
    "# Create the weight array of size (3 x 4) as we have 3 inputs and 4 desired outputs\n",
    "weights = np.array([[1., 1., 1., 1.],\n",
    "                    [1., 1., 1., 1.],\n",
    "                    [1., 1., 1., 1.]])\n",
    "\n",
    "# Perform matrix multiplication on the inputs and weights, note np.dot() is the standard and combined way of performing both dot products (which are normally for vectors) and matrix multiplication (which is for matrices)\n",
    "outputs = np.dot(inputs, weights)\n",
    "\n",
    "print(f\"The matrix mult. output is: {outputs}\")"
   ],
   "id": "f0f6b13e50370a9e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The matrix mult. output is: [7. 7. 7. 7.]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "That should about give you an intuition of how a neural network layer really works, but there's one more thing to address in this part of things: biases. \n",
    "\n",
    "Biases are parameters which are trainable and which are meant to offset the outputs positively or negatively. Each neuron has its own bias parameter, which we usually initialize biases to 0 and then modify according to the gradients during training. It's really quite a simple concept and there's not much more to it.\n",
    "\n",
    "In practice, these biases are added as scalars (integers) to the product of each neuron. Let's look at how this happens, using our inputs and weights from the previous code cell."
   ],
   "id": "bd6a70a6ed24e848"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T00:22:43.205424Z",
     "start_time": "2025-06-26T00:22:43.196390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# INPUTS AND WEIGHTS ARE PRE-LOADED FROM THE EARLIER CELL\n",
    "\n",
    "# NEW: create the biases \n",
    "biases = np.array([1., 2., 3., 4.])\n",
    "\n",
    "# Perform matrix multiplication on the inputs and weights, note np.dot() is the standard and combined way of performing both dot products (which are normally for vectors) and matrix multiplication (which is for matrices)\n",
    "outputs = np.dot(inputs, weights)\n",
    "\n",
    "print(f\"The outputs before adding biases are: {outputs}\")\n",
    "\n",
    "outputs += biases\n",
    "\n",
    "print(f\"The outputs after adding biases are: {outputs}\")"
   ],
   "id": "d25c038fa33f2efd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The outputs before adding biases are: [7. 7. 7. 7.]\n",
      "The outputs after adding biases are: [ 8.  9. 10. 11.]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now, we understand how the basics of how the whole thing works, at least in the forward pass. All in all, each neuron is really just carrying out ```y=mx+b``` where m is the input, x is the weight, b is the neuron's bias, and the result y is the neuron's output.\n",
    "\n",
    "Great, so we can now implement the forward class of our dense layer. Like I said above, I'll implement an example class here but all the changes will be saved in the corresponding classes.py file."
   ],
   "id": "c0bb05e32d2f80bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T00:39:50.824437Z",
     "start_time": "2025-06-26T00:39:50.817365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class exDenseLayer:\n",
    "    # Initialization method to initialize a dense layer object\n",
    "    def __init__(self, numInputs, numNeurons):\n",
    "        # Create random weights and scale them down, of size (inputs x neurons)\n",
    "        self.weights = 0.01 * np.random.randn(numInputs, numNeurons)\n",
    "        # Create an array filled with 0's with a bias for each neuron\n",
    "        self.biases = np.zeros((1, numNeurons))\n",
    "    \n",
    "    # Our forward pass method\n",
    "    def forward(self, inputs):\n",
    "        # Calculate outputs using the method previously discussed\n",
    "        self.outputs = np.dot(inputs, self.weights) + self.biases"
   ],
   "id": "fc1a48999366a609",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "That's about everything we need for a forward pass! Now we get to talk about the ReLU activation function!",
   "id": "10068d7f1f0cdfdb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
