{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Part 1: Implementing a feedforward neural network using NumPy\n",
    "\n",
    "We'll implement a neural network on the MNIST dataset -- from scratch only using the Numpy library. That will allow us to build an intuition of how our model actually works beyond the surface level PyTorch implementation that we'll do later in this project. \n",
    "\n",
    "Like I said, this will be a feedforward neural network, meaning information only flows forward during inference. To do this, we'll make use of dense layers, the ReLU activation function, categorical cross-entropy loss, and a softmax activation function for the output. I've made a little diagram of what we're going to be implementing below:\n",
    "> INSERT IMG\n",
    "\n",
    "I'll work on everything in sections, implementing it with example classes in this jupyter notebook, and adding the real classes that we'll use into the classes.py file in this same folder (filepath: fnn/p1-numpy/classes.py).\n",
    "\n",
    "As a precursor to all our work, let's initialize the necessary packages in the workspace. "
   ],
   "id": "27bc1f75b0927bc9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:01:22.221111Z",
     "start_time": "2025-06-27T02:01:22.066596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Package Initialization\n",
    "import math\n",
    "import numpy as np\n",
    "from classes import DenseLayer, ReLU, Softmax, CategoricalCrossEntropyLoss"
   ],
   "id": "1e65ce53eb3e3a81",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we can begin the implementation :)\n",
    "\n",
    "Dense layers, otherwise known as fully-connected layers, are the foundation of neural networks. A dense layer is really quite simple, if you understand the workings of matrix multiplication. I'll briefly go over it, but if you need a better introduction to dense layers I would recommend the book *Neural Networks from Scratch* by Kinsley and Kukie≈Ça.\n",
    "\n",
    "# Section 1: The Forward Pass of Dense Layers\n",
    "\n",
    "At the core level, the way a neural network layer works is matrix multiplication. We can work through an example for this. Let's think of this as creating a function where we input a list X, of length 3 with integers, and it outputs a list Y, of length 4 with sum(X) in each entry. You can look at the diagram below for an example:\n",
    "\n",
    "<br/>\n",
    "\n",
    "<img src=\"../../diagrams/ex1.png\" alt=\"A diagram showing our proposed 'function.'\"/>\n",
    "\n",
    "Skip this if you understand matrix multiplication: effectively, what we're doing is just a function that carries out matrix multiplication. Matrix multiplication is done by the element wise multiplication of two matrices of sizes (n x m) and (m x p), respectively. As you can see there, that middle dimension adding up is key, and that's because we're doing row x column. I won't go too into the weeds there, but let's look at a mini-example -- almost recursive if that makes sense :). Let's say we're trying to multiply a matrix X of size (1 x 2) times another matrix Y of size (2 x 2). That can be done because the middle dimension lines up. If matrix X is [a, b] and matrix Y is [[c, d],[e, f]]. That is, where every column of matrix Y represents the weights of one neuron as a column vector. Then, what matrix multiplication is doing is just [(a\\*c + a\\*e), (b\\*d + b\\*f)]. That's it for this recursive example, and I hope that makes sense.\n",
    "\n",
    "Now, I'll quickly use Numpy to show the main example I'm referring to above. Please note, np.array() is the standard way for creating arrays or matrices in numpy. "
   ],
   "id": "901a6e2208f729b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T23:53:43.938345Z",
     "start_time": "2025-06-25T23:53:43.930773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create the input array of size (1 x 3)\n",
    "inputs = np.array([1., 2., 4.])\n",
    "# Create the weight array of size (3 x 4) as we have 3 inputs and 4 desired outputs\n",
    "weights = np.array([[1., 1., 1., 1.],\n",
    "                    [1., 1., 1., 1.],\n",
    "                    [1., 1., 1., 1.]])\n",
    "\n",
    "# Perform matrix multiplication on the inputs and weights, note np.dot() is the standard and combined way of performing both dot products (which are normally for vectors) and matrix multiplication (which is for matrices)\n",
    "outputs = np.dot(inputs, weights)\n",
    "\n",
    "print(f\"The matrix mult. output is: {outputs}\")"
   ],
   "id": "f0f6b13e50370a9e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The matrix mult. output is: [7. 7. 7. 7.]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "That should about give you an intuition of how a neural network layer really works, but there's one more thing to address in this part of things: biases. \n",
    "\n",
    "Biases are parameters which are trainable and which are meant to offset the outputs positively or negatively. Each neuron has its own bias parameter, which we usually initialize biases to 0 and then modify according to the gradients during training. It's really quite a simple concept and there's not much more to it.\n",
    "\n",
    "In practice, these biases are added as scalars (integers) to the product of each neuron. Let's look at how this happens, using our inputs and weights from the previous code cell."
   ],
   "id": "bd6a70a6ed24e848"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T00:22:43.205424Z",
     "start_time": "2025-06-26T00:22:43.196390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# INPUTS AND WEIGHTS ARE PRE-LOADED FROM THE EARLIER CELL\n",
    "\n",
    "# NEW: create the biases \n",
    "biases = np.array([1., 2., 3., 4.])\n",
    "\n",
    "# Perform matrix multiplication on the inputs and weights, note np.dot() is the standard and combined way of performing both dot products (which are normally for vectors) and matrix multiplication (which is for matrices)\n",
    "outputs = np.dot(inputs, weights)\n",
    "\n",
    "print(f\"The outputs before adding biases are: {outputs}\")\n",
    "\n",
    "outputs += biases\n",
    "\n",
    "print(f\"The outputs after adding biases are: {outputs}\")"
   ],
   "id": "d25c038fa33f2efd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The outputs before adding biases are: [7. 7. 7. 7.]\n",
      "The outputs after adding biases are: [ 8.  9. 10. 11.]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now, we understand how the basics of how the whole thing works, at least in the forward pass. All in all, each neuron is really just carrying out ```y=mx+b``` where m is the input, x is the weight, b is the neuron's bias, and the result y is the neuron's output.\n",
    "\n",
    "Great, so we can now implement the forward class of our dense layer. Like I said above, I'll implement an example class here but all the changes will be saved in the corresponding classes.py file."
   ],
   "id": "c0bb05e32d2f80bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T00:39:50.824437Z",
     "start_time": "2025-06-26T00:39:50.817365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class exDenseLayer:\n",
    "    # Initialization method to initialize a dense layer object\n",
    "    def __init__(self, numInputs, numNeurons):\n",
    "        # Create random weights and scale them down, of size (inputs x neurons)\n",
    "        self.weights = 0.01 * np.random.randn(numInputs, numNeurons)\n",
    "        # Create an array filled with 0's with a bias for each neuron\n",
    "        self.biases = np.zeros((1, numNeurons))\n",
    "    \n",
    "    # Our forward pass method\n",
    "    def forward(self, inputs):\n",
    "        # Calculate outputs using the method previously discussed\n",
    "        self.outputs = np.dot(inputs, self.weights) + self.biases"
   ],
   "id": "fc1a48999366a609",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "That's about everything we need for a forward pass! Now we get to talk about the ReLU activation function!\n",
    "\n",
    "## Section 2: The Various Activation Functions\n",
    "Activation functions are necessary parts of neural networks if we ever want to model non-linear functions. In this section, we'll cover two of them: ReLU and Softmax.\n",
    "\n",
    "### Section 2.1: The ReLU Activation Function\n",
    "\n",
    "ReLU stands for \"rectified linear unit\" and is a piecewise activation function that makes the best of all worlds. It basically works using the following equation:\n",
    "$$\n",
    "f(x) = \n",
    "\\begin{cases}\n",
    "    x & \\text{ if } x > 0 \\\\\n",
    "    0 & \\text{ otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Let's implement a class for our ReLU activation function, making use of the properties of Numpy to simplify our work."
   ],
   "id": "10068d7f1f0cdfdb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T01:12:07.327763Z",
     "start_time": "2025-06-26T01:12:07.317633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class exReLU:\n",
    "    # Our forward method; no overriding init method needed\n",
    "    def forward(self, inputs):\n",
    "        # Using the np.maximum() method to carry out our comparisons \n",
    "        self.output = np.maximum(0, inputs)"
   ],
   "id": "cfc3d12dc36f305b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, let's try it out on an example array to see it in action. Spoiler, it's nothing crazy.",
   "id": "eeec50d0ecc7cfef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T01:12:16.662523Z",
     "start_time": "2025-06-26T01:12:16.657080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a random array\n",
    "exOutput = np.array([[-0.5, 0.3],\n",
    "                   [0.43, -0.1]])\n",
    "\n",
    "# Instantiate a ReLU object\n",
    "activation = ReLU()\n",
    "\n",
    "# Use the ReLU on our example outputs\n",
    "activation.forward(exOutput)\n",
    "\n",
    "print(activation.output)"
   ],
   "id": "3fd9813cb50af695",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.3 ]\n",
      " [0.43 0.  ]]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As we can see, the values less than 0 have been zerod out, whereas all the non-negative values have remained as they were!\n",
    "\n",
    "And that's all there is to ReLU. It basically zeros out values less than 0 and keeps values greater than or equal to 0. Now we can move to the second activation function we'll look at: Softmax.\n",
    "\n",
    "### Section 2.2: Softmax\n",
    "\n",
    "The Softmax activation function is the one that we'll be using on our outputs. The Softmax is used in situations where we're to perform some kind of categorical operations. It works by taking the outputs and creating probabilities for each output class that ultimately add up to one -- giving us the \"most likely\" class.\n",
    "\n",
    "The function for the Softmax function is:\n",
    "$$\n",
    "\\frac{e^{z_{i,j}}}{\\sum_{l=1}^{L} e^{z_{i,l}}}\n",
    "$$\n",
    "\n",
    "All that really means is it's getting the normalized probabilities for each, with it all adding up to 1.\n",
    "\n",
    "So, let's implement this in a class. "
   ],
   "id": "778d95ac6b7bdc85"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T01:45:08.652602Z",
     "start_time": "2025-06-26T01:45:08.631727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class exSoftmax:\n",
    "    # Method for the forward pass; no init overriding needed\n",
    "    def forward(self, inputs):\n",
    "        # Calculate the non-normalized probabilities using np.exp(), which exponentiates everything, np.max() which finds the maximum value in the matrix, and np.sum() which performs a summation of each row/col as desired\n",
    "        unProb = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        # Normalize the inputs for every sample\n",
    "        nProb = unProb / np.sum(unProb, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = nProb"
   ],
   "id": "e3b27e35ce7d63d0",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "With that implemented, let's demonstrate how it works! ",
   "id": "e54745119ac4d262"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T01:52:16.374896Z",
     "start_time": "2025-06-26T01:52:16.368427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create an array with hypothetical outputs\n",
    "exOutput = np.array([[1, 4, 5]])\n",
    "\n",
    "# Instantiate our Softmax object\n",
    "activation = Softmax()\n",
    "\n",
    "# Run our output through a Softmax layer\n",
    "activation.forward(exOutput)\n",
    "\n",
    "print(f\"The normalized output probabilities are: {activation.output}\")"
   ],
   "id": "36d3c77d25ac4a2f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The normalized output probabilities are: [[0.01321289 0.26538793 0.72139918]]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Awesome, and that's really it for our Softmax. There's not much more we need to know for the purposes of this implementation. Now, we're going to move to our method of measuring accuracy: the categorical cross-entropy loss.\n",
    "\n",
    "## Section 3: Categorical Cross-Entropy Loss\n",
    "\n",
    "Categorical cross entropy loss is a loss function which is used to compare a ground-truth probability (y, or targets) with some predicted distribution (y-hat, or predictions). It's one of the most commonly used loss functions with a softmax on the output, because it allows us to perform classification.\n",
    "\n",
    "The formula for calculating the categorical cross-entropy is:\n",
    "$$\n",
    "L_{i} = - log(\\hat{y}_{i,k})\n",
    "$$\n",
    "That is, where i denotes the sample loss value, i is the i-th sample in the set, k is the index of the target label, y denotes the target values, and y-hat denotes the predicted values.\n",
    "\n",
    "Generally, we apply a log loss kind of function to the outputs of a binary log regression. That means, one where there are only two classes in the distribution. We apply this in our example where the target output is one-hot encoded, meaning there is only 0 or 1.\n",
    "\n",
    "So, let's create our class to do this!"
   ],
   "id": "f842eefdcd9762e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:01:55.158338Z",
     "start_time": "2025-06-27T02:01:55.150116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class exCategoricalCrossEntropyLoss:\n",
    "    # The forward method because we don't need to override any initializations\n",
    "    def forward(self, yPred, yTrue):\n",
    "        # Number of samples in the batch\n",
    "        samples = len(yPred)\n",
    "        # Clip our data from both ends to prevent division by 0\n",
    "        yPred = np.clip(yPred, 1e-7, 1-1e-7)\n",
    "        \n",
    "        # Probabilities for one-hot encoded target values\n",
    "        confidences = np.sum(yPred * yTrue, axis=1)\n",
    "        # Losses\n",
    "        negLogLikelihood = -np.log(confidences)\n",
    "        \n",
    "        return negLogLikelihood\n",
    "        \n",
    "    # A calculate method\n",
    "    def calculate(self, output, y):\n",
    "        # Calculate the loss\n",
    "        sampleLoss = self.forward(output, y)\n",
    "        # Calculate the mean loss\n",
    "        dataLoss = np.mean(sampleLoss)\n",
    "        \n",
    "        return dataLoss"
   ],
   "id": "7b8c02fdb32d216f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "That's all we need for our categorical cross entropy class. Let's test it out to see how it works!",
   "id": "9a1ec3ac0744889e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T02:15:42.818821Z",
     "start_time": "2025-06-27T02:15:42.796394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the loss object\n",
    "lossFunction = CategoricalCrossEntropyLoss()\n",
    "\n",
    "# Test one, totally accurate predictions.\n",
    "aPred = np.array([[0, 1, 0],\n",
    "                  [1, 0, 0]])\n",
    "aTrue = np.array([[0, 1, 0],\n",
    "                  [1, 0, 0]])\n",
    "print(f\"The total loss is {lossFunction.calculate(aPred, aTrue):.3f}, which should be 0.\")\n",
    "\n",
    "# Test two, medium but correct accuracy predictions.\n",
    "aPred = np.array([[.4, .6, 0],\n",
    "                  [.2, .7, .1]])\n",
    "aTrue = np.array([[0, 1, 0],\n",
    "                  [0, 1, 0]])\n",
    "print(f\"The total loss is {lossFunction.calculate(aPred, aTrue):.3f}, which should be nonzero.\")\n",
    "\n",
    "# Test three, low but correct accuracy predictions.\n",
    "aPred = np.array([[.4, .3, .3],\n",
    "                  [.33, .34, .33]])\n",
    "aTrue = np.array([[1, 0, 0],\n",
    "                  [0, 1, 0]])\n",
    "print(f\"The total loss is {lossFunction.calculate(aPred, aTrue):.3f}, which should be nonzero.\")\n",
    "\n",
    "# Test four, totally wrong predictions.\n",
    "aPred = np.array([[1, 0, 0],\n",
    "                  [0, 1, 0]])\n",
    "aTrue = np.array([[0, 0, 1],\n",
    "                  [1, 0, 0]])\n",
    "print(f\"The total loss is {lossFunction.calculate(aPred, aTrue):.3f}, which should be very high.\")\n"
   ],
   "id": "1350466f04423c9e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total loss is 0.000, which should be 0.\n",
      "The total loss is 0.434, which should be nonzero.\n",
      "The total loss is 0.998, which should be nonzero.\n",
      "The total loss is 16.118, which should be very high.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As you can see, there's a very low loss (0) when the model makes totally accurate predictions, which begins to go up as predictions get less accurate. That's about all we need to know.\n",
    "\n",
    "## Section 4: Backpropagation & Gradients\n",
    "\n",
    "I've waited to introduce all the underlying concepts so that we can work through the idea of backpropagation with a complete picture of how a forward pass really works. \n",
    "\n",
    "From the surface level, backpropagation may seem to be the most complex part of our neural network -- but I'll try to do my best to explain it succinctly. \n",
    "\n",
    "At the core of backpropagation lies the principle of the chain rule, from calculus. First, let's give an example of how a single neuron sums up its inputs and applies an activation function, from which we can then have an intuition of how the chain rule applies.\n",
    "\n",
    "<br/>\n",
    "\n",
    "<img src=\"../../diagrams/ex2.png\" alt=\"A diagram showing our proposed 'function.'\"/>\n",
    "\n",
    "In the above image, it shows how a neuron would process inputs and apply the ReLU activation function to them. It's pretty simple, and the red numbers are values passed through the system, whereas black numbers are parameters.\n",
    "\n",
    "When we look at it from right to left, we're using the outputs of previous actions to complete the next step. So, what that means here is: \n",
    "$$\n",
    "ReLU(sum(i_{0} \\times w_{0}, i_{1} \\times w_{1}, i_{2} \\times w_{2}, i_{3} \\times w_{3}, bias))\n",
    "$$\n",
    "That is, where $i_{x}$ is the x'th input and $w_{x}$ is the x'th weight.\n",
    "\n",
    "Therefore, for us to find the derivative of this, we need to apply the chain rule, which states that:\n",
    "$$\n",
    "\\frac{d}{dx}[f(g(x))] = f'(g(x)) \\times g'(x)\n",
    "$$ \n",
    "We keep applying this chain rule recursively to find the gradients -- which are vectors of partial derivatives with respect to each variable.\n",
    "\n",
    "I won't dive much deeper into partial derivatives, but I would recommend that you pick up a book like Neural Networks from Scratch by Kinsley and Kukie≈Ça to learn more about it :). Now, we need to implement the backward pass methods for each component!\n",
    "\n",
    "Let's go back in the same order which we implemented things initially.\n"
   ],
   "id": "7d84e4751618d9a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e9b8dc7bb36b3bc7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
